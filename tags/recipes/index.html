<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.0.1">
<title data-rh="true">2 posts tagged with &quot;recipes&quot; | Responsible AI Cookbook</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://30DaysOf.github.io/responsible-ai/img/banners/000-roadmap.png"><meta data-rh="true" name="twitter:image" content="https://30DaysOf.github.io/responsible-ai/img/banners/000-roadmap.png"><meta data-rh="true" property="og:url" content="https://30DaysOf.github.io/responsible-ai/tags/recipes"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="2 posts tagged with &quot;recipes&quot; | Responsible AI Cookbook"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/responsible-ai/img/favicon.ico"><link data-rh="true" rel="alternate" href="https://30DaysOf.github.io/responsible-ai/tags/recipes" hreflang="en"><link data-rh="true" rel="alternate" href="https://30DaysOf.github.io/responsible-ai/tags/recipes" hreflang="x-default"><link data-rh="true" rel="canonical" href="e-ai-tools-to-help-operationalize-responsible-ai-for-generative-ai-apps-11ig"><link rel="alternate" type="application/rss+xml" href="/responsible-ai/rss.xml" title="Responsible AI Cookbook RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/responsible-ai/atom.xml" title="Responsible AI Cookbook Atom Feed"><link rel="stylesheet" href="/responsible-ai/assets/css/styles.2c45f084.css">
<script src="/responsible-ai/assets/js/runtime~main.1fed7dcd.js" defer="defer"></script>
<script src="/responsible-ai/assets/js/main.3a5b4236.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbar--primary"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/responsible-ai/"><div class="navbar__logo"><img src="/responsible-ai/img/logo.png" alt="Responsible AI Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/responsible-ai/img/logo.png" alt="Responsible AI Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Responsible AI Cookbook</b></a></div><div class="navbar__items navbar__items--right"><a href="https://dev.to/nitya/series/25819" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">This Week In AI</a><a href="https://aka.ms/rai-hub/collection" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Collection</a><a href="https://github.com/30DaysOf/responsible-ai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">My Recent Posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/responsible-ai/twain-apr01-2024">Responsible AI On Azure</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/responsible-ai/cookbook-kickoff">My Responsible AI Cookbook</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="https://schema.org/Blog"><header class="margin-bottom--xl"><h1>2 posts tagged with &quot;recipes&quot;</h1><a href="/responsible-ai/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting"><meta itemprop="description" content="Last week, we saw the announcement about a new set of #ResponsibleAI tools in Azure AI, to build more secure and trustworthy generative AI applications. In this post, we&#x27;ll take a tour of the tools announced with links for deeper dives"><meta itemprop="keywords" content="azure,ai-studio,responsible-ai,generative-ai,rai-dashboard,rai-evaluation,rai-content-safety,rai-cookbook"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/responsible-ai/twain-apr01-2024">Responsible AI On Azure</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-04-01T00:00:00.000Z" itemprop="datePublished">April 1, 2024</time> Â· <!-- -->7 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/nitya" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/nitya.png" alt="Nitya Narasimhan" itemprop="image"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/nitya" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Nitya Narasimhan</span></a></div><small class="avatar__subtitle" itemprop="description">PhD &amp; Polyglot, AI @Microsoft</small></div></div></div></div></header><div class="markdown" itemprop="articleBody">
<p><img loading="lazy" src="https://media.dev.to/cdn-cgi/image/width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F202dnkp100nlo2eoe41v.png" alt="Banner" class="img_ev3q"></p>
<p><em>This post was originally publishd on dev.to as part of my This Week In AI News series</em>.</p>
<hr>
<p>We&#x27;ve talked about #ResponsibleAI in two contexts before - <a href="https://dev.to/azure/train-debug-ml-models-for-responsible-ai-join-the-aiskillschallenge-3pb3" target="_blank" rel="noopener noreferrer">Model Debugging</a> for predictive AI apps (MLOps), and <a href="https://dev.to/azure/fuel-your-intelligent-apps-with-azure-ai-3j4b" target="_blank" rel="noopener noreferrer">AI-Assisted Evaluation</a> for generative AI (LLMOps). Then late last week, I shared <a href="https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/" target="_blank" rel="noopener noreferrer">this exciting announcement</a> about new Responsible AI tools coming to Azure AI!</p>
<p>In today&#x27;s post, I want to dig a little deeper into the announcement to learn what the tools do, why they matter, and how developers can get started using them in their generative AI application workflows.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1--the-azure-ai-platform">1 | The Azure AI Platform<a href="#1--the-azure-ai-platform" class="hash-link" aria-label="Direct link to 1 | The Azure AI Platform" title="Direct link to 1 | The Azure AI Platform">â</a></h2>
<p>The <a href="https://ai.azure.com" target="_blank" rel="noopener noreferrer">Azure AI Studio</a> provides a browser-based UI/UX for exploring the rich capabilities of the Azure AI plaform as shown below. It also supports <a href="https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/a-code-first-experience-for-building-a-copilot-with-azure-ai/ba-p/4058659" target="_blank" rel="noopener noreferrer">a code-first experience</a> for developers who prefer working with an SDK or command-line tools.</p>
<p><img loading="lazy" src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/mq4e73jqikkj2bnbnpv4.png" alt="Azure AI Studio" class="img_ev3q"></p>
<p>It streamlines your end-to-end development workflow for generative AI applications - from exploring the model catalog, to building &amp; evaluating your AI application, to deploying and monitoring the application in production. With built-in support for <em>operationalizing Responsible AI</em>, developers can go from evaluating their applications for quality, to configuring them for content safety in production.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2--new-azure-ai-tools-for-responsible-ai">2 | New Azure AI Tools for Responsible AI<a href="#2--new-azure-ai-tools-for-responsible-ai" class="hash-link" aria-label="Direct link to 2 | New Azure AI Tools for Responsible AI" title="Direct link to 2 | New Azure AI Tools for Responsible AI">â</a></h2>
<p>The <a href="https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/" target="_blank" rel="noopener noreferrer">recent announcement</a> from the Responsible AI team highlights a number of new tools and capabilities that are available (or coming soon) to Azure AI, to further improve the quality and safety of generative AI application on Azure. This short video gives you a quick preview of how these tools are put to use to create safeguards for generative AI apps on Azure. In the rest of this post, we&#x27;ll dive briefly into each of these tools to understand what they do, and why it matters.</p>
<iframe width="600" height="400" frameborder="0" src="https://www.youtube.com/embed/BnGmozWvsOo" title="How to Safeguard your generative AI applications in Azure AI"></iframe>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3--prompt-shields">3 | Prompt Shields<a href="#3--prompt-shields" class="hash-link" aria-label="Direct link to 3 | Prompt Shields" title="Direct link to 3 | Prompt Shields">â</a></h2>
<p><img loading="lazy" src="https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/03/image-5.webp" alt="Prompt Shields" class="img_ev3q"></p>
<p>The first new capability comes in the form of <a href="https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-announces-prompt-shields-for-jailbreak-and-indirect/ba-p/4099140" target="_blank" rel="noopener noreferrer">Prompt Shields</a> that can <strong>detect and block prompt injection attacks</strong> to safeguard the <em>integrity</em> of your LLM system. These attacks work by tricking the system into harmful or unplanned behaviors, &quot;injecting&quot; unauthorized instructions into the default user prompt at runtime.</p>
<p>In a <strong>direct attack</strong> (jailbreak) the user is the adversary. The user prompt attempts to get the model to disregard developer-authored system prompts and training in favor of executing potentially harmful instructions. In an <strong>indirect attack</strong> (cross-domain prompt injection) the adversary is a third-party and the attack occurs via untrusted external data sources that may be embedded in the user prompt, but not authored by user or developer.</p>
<p>Prompt Shields work proactively to detect suspicious inputs in real-time and block them before they reach the LLM. This can use techniques like <a href="https://arxiv.org/abs/2403.14720" target="_blank" rel="noopener noreferrer">Spotlighting</a> that transform the input to mitigate these attacks while preserving the semantic content of the user prompt.</p>
<p>ð | <a href="https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-announces-prompt-shields-for-jailbreak-and-indirect/ba-p/4099140" target="_blank" rel="noopener noreferrer"><strong>Learn more in this post</strong></a>
ð | <a href="https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/" target="_blank" rel="noopener noreferrer"><strong>Review the main announcement</strong></a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="4--groundedness-detection">4 | Groundedness Detection<a href="#4--groundedness-detection" class="hash-link" aria-label="Direct link to 4 | Groundedness Detection" title="Direct link to 4 | Groundedness Detection">â</a></h2>
<p><img loading="lazy" src="https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/03/Groundednes-Detection.webp" alt="Groundedness Detection" class="img_ev3q"></p>
<p>The second capability involves <a href="https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/detect-and-mitigate-ungrounded-model-outputs/ba-p/4099261" target="_blank" rel="noopener noreferrer"><strong>Groundedness Detection</strong></a> to combat the familiar problem of <em>&quot;Hallucinations&quot;</em>. Here, models fabricate a response that may look valid but is not grounded in any real data. Identifying and remediating this is critical to improve <strong>trustworthiness</strong> of generative AI responses.</p>
<p>Previously developer options included manual checks (not scalable) and chaining requests (to have an LLM evaluate if the previous response was grounded with respect to a reference document) with mixed results. The new tool uses a custom-built fine-tuned language model that detects <em>ungrounded claims</em> more accurately - giving developers multiple options to mitigate the behavior, from pre-deployment testing to post-deployment rewriting of responses.</p>
<p>ð | <a href="https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/detect-and-mitigate-ungrounded-model-outputs/ba-p/4099261" target="_blank" rel="noopener noreferrer"><strong>Learn more in this post</strong></a>
ð | <a href="https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/" target="_blank" rel="noopener noreferrer"><strong>Review the main announcement</strong></a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="5--safety-system-messages">5 | Safety System Messages<a href="#5--safety-system-messages" class="hash-link" aria-label="Direct link to 5 | Safety System Messages" title="Direct link to 5 | Safety System Messages">â</a></h2>
<p>The third capability recognizes that prompt engineering is a powerful way to improve the reliability of the generative AI application, along with services like Azure AI Content Safety. Writing <strong>effective system prompts</strong> (metaprompts) can have a non-trivial impact on the quality of responses - and system messages that can &quot;guide the optimal use of grounding data and overall behavior&quot; are ideal.</p>
<p><img loading="lazy" src="https://learn.microsoft.com/en-us/azure/ai-services/openai/media/concepts/system-message/template.png#lightbox" alt="Safety System Messages" class="img_ev3q"></p>
<p>With the new <a href="https://learn.microsoft.com/azure/ai-services/openai/concepts/system-message" target="_blank" rel="noopener noreferrer">system message framework and template recommendations for LLMs</a>, developers now get example templates and recommendations to help them craft more effective messages. For instance, the system message framework describes four concepts (define model capabilities, define model output format, provide examples, provide behavioral guardrails) you can apply in crafting the system message. The screenshot above shows an example of how this is applied in a retail chatbot app.</p>
<p>ð | <a href="https://learn.microsoft.com/azure/ai-services/openai/concepts/system-message" target="_blank" rel="noopener noreferrer"><strong>Learn more from the documentation</strong></a>
ð | <a href="https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/" target="_blank" rel="noopener noreferrer"><strong>Review the main announcement</strong></a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="6--automated-safety-evaluations">6 | Automated Safety Evaluations<a href="#6--automated-safety-evaluations" class="hash-link" aria-label="Direct link to 6 | Automated Safety Evaluations" title="Direct link to 6 | Automated Safety Evaluations">â</a></h2>
<p><img loading="lazy" src="https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/03/image-4.webp" alt="Safety Evaluations" class="img_ev3q"></p>
<p>The fourth capability recognizes that most developers lack the resources and expertise to conduct rigorous safety evaluations on their generative AI applications - which would involve curating high-quality datasets for testing, and interpreting evaluation results for effective mitigation.</p>
<p>Previously, Azure AI supported pre-built <a href="https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in?tabs=warning#generation-quality-metrics" target="_blank" rel="noopener noreferrer">generation quality metrics</a> like <em>groundedness</em>, <em>relevance</em>, <em>coherence</em> and <em>fluency</em> for AI-assisted evaluations. With the new capability, this now includes additional <a href="https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in?tabs=warning#risk-and-safety-metrics" target="_blank" rel="noopener noreferrer">risk and safety metrics</a> like <em>hateful and unfair content</em>, <em>sexual content</em>, <em>violent content</em>, <em>self-harm-related content</em>, and jailbreaks.</p>
<p><img loading="lazy" src="https://techcommunity.microsoft.com/t5/image/serverpage/image-id/565820iA1D8020BB82AD593/image-size/large?v=v2&amp;px=999" alt="Safety Evaluation Workflow" class="img_ev3q"></p>
<p>To conduct a safety evaluation on your generative AI application, you need a test dataset and some way to simulate adversarial interactions with your application so you can evaluate the resulting responses for the relevant safety metrics. The new Azure AI <strong>automated safety evaluations</strong> capability streamlines this for you in four steps:</p>
<ul>
<li>Start with targeted prompts (created from templates)</li>
<li>Use AI-assisted simulation (for adversarial interactions)</li>
<li>Create your test datasets (baseline &amp; adversarial)</li>
<li>Evaluate the test datasets (for your application)
Â 
Outcomes can now be used to configure or adapt other elements of the application&#x27;s risk mitigation system.</li>
</ul>
<p>ð | <a href="https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/introducing-ai-assisted-safety-evaluations-in-azure-ai-studio/ba-p/4098595" target="_blank" rel="noopener noreferrer"><strong>Learn more in this post</strong></a>
ð | <a href="https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/" target="_blank" rel="noopener noreferrer"><strong>Review the main announcement</strong></a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="7--risk--safety-monitoring">7 | Risk &amp; Safety Monitoring<a href="#7--risk--safety-monitoring" class="hash-link" aria-label="Direct link to 7 | Risk &amp; Safety Monitoring" title="Direct link to 7 | Risk &amp; Safety Monitoring">â</a></h2>
<p><img loading="lazy" src="https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/03/image-6.webp" alt="Risk and Safety Monitoring" class="img_ev3q"></p>
<p>The final new capability announced was around <a href="https://learn.microsoft.com/azure/ai-services/openai/how-to/risks-safety-monitor" target="_blank" rel="noopener noreferrer">Risk &amp; Safety Monitoring in Azure Open AI</a> - adding a new Dashboard capability described as follows:</p>
<blockquote>
<p>In addition to the detection/ mitigation on harmful content in near-real time, the risks &amp; safety monitoring help get a better view of how the content filter mitigation works on real customer traffic and provide insights on potentially abusive end-users.</p>
</blockquote>
<p>To use the feature, you need an Azure OpenAI resource in a supported region, and a model deployment with a content filter configured. Once setup, simply open the <strong>Deployments</strong> tab, visit the model deployment page, and select the <strong>Risks &amp; Safety</strong> tab as shown in this figure from the announcement post below.</p>
<p><img loading="lazy" src="https://techcommunity.microsoft.com/t5/image/serverpage/image-id/565909i547F8DDB0124F9BC/image-dimensions/554x621?v=v2" alt="Filter tab" class="img_ev3q"></p>
<p>The resulting dashboard provides two kinds of insights into content filter effectiveness. The first focuses on <strong>Content Detection</strong> with visualized insights into metrics like <em>Total blocked request count and block rate</em>, <em>Blocked requests by category</em>, <em>Severity distribution by category</em> and more. The second focuses on <strong>Abusive User Detection</strong> to highlight how regularly the content filters safeguards are abused by end users and identify the severity and frequency of those occurrences.</p>
<p>ð | <a href="https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/introducing-risks-amp-safety-monitoring-feature-in-azure-openai/ba-p/4099218" target="_blank" rel="noopener noreferrer"><strong>Learn more in this post</strong></a>
ð | <a href="https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/" target="_blank" rel="noopener noreferrer"><strong>Review the main announcement</strong></a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="8--get-started-exploring">8 | Get Started Exploring<a href="#8--get-started-exploring" class="hash-link" aria-label="Direct link to 8 | Get Started Exploring" title="Direct link to 8 | Get Started Exploring">â</a></h2>
<p>This was a lot - and it is still just the tip of the iceberg when it comes to actively understanding and applying responsible AI principles in practice. Want to get started exploring this topic furthere? Bookmark and revisit these three core resources:</p>
<table><thead><tr><th style="text-align:left">Resource</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left">1ï¸â£ <a href="https://azure.microsoft.com/en-us/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/" target="_blank" rel="noopener noreferrer">Blog Post</a></td><td style="text-align:left">Official Announcement of New Tools</td></tr><tr><td style="text-align:left">2ï¸â£ <a href="https://aka.ms/rai-hub/collection" target="_blank" rel="noopener noreferrer">Collection</a></td><td style="text-align:left">My Responible AI For Developers Collection</td></tr><tr><td style="text-align:left">3ï¸â£ <a href="https://aka.ms/ai-studio/collection" target="_blank" rel="noopener noreferrer">Collection</a></td><td style="text-align:left">My Azure AI For Developers Collection</td></tr></tbody></table>
<hr></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/responsible-ai/tags/responsibleai">responsibleai</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/responsible-ai/tags/principles">principles</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/responsible-ai/tags/tools">tools</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/responsible-ai/tags/practices">practices</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/responsible-ai/tags/recipes">recipes</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/responsible-ai/tags/recipes/page/2"><div class="pagination-nav__label">Older Entries</div></a></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a href="https://responsibleaitoolbox.ai/" target="_blank" rel="noopener noreferrer" class="footer__link-item">RAI Toolbox</a><span class="footer__link-separator">Â·</span><a href="https://aka.ms/rai-hub/collection" target="_blank" rel="noopener noreferrer" class="footer__link-item">RAI Collection</a><span class="footer__link-separator">Â·</span><a href="https://aka.ms/rai-hub/website" target="_blank" rel="noopener noreferrer" class="footer__link-item">RAI Developer Hub</a><span class="footer__link-separator">Â·</span><a href="https://learn.microsoft.com/ai/" target="_blank" rel="noopener noreferrer" class="footer__link-item">AI Developer Hub</a><span class="footer__link-separator">Â·</span><a href="https://discord.gg/yrTeVQwpWm" target="_blank" rel="noopener noreferrer" class="footer__link-item">Azure AI Discord</a><span class="footer__link-separator">Â·</span><a href="https://github.com/nitya" target="_blank" rel="noopener noreferrer" class="footer__link-item">Â© 2024 Nitya Narasimhan</a></div></div></div></footer></div>
</body>
</html>