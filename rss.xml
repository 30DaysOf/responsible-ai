<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Responsible AI Cookbook Blog</title>
        <link>https://30DaysOf.github.io/responsible-ai/</link>
        <description>Responsible AI Cookbook Blog</description>
        <lastBuildDate>Mon, 01 Apr 2024 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Responsible AI On Azure]]></title>
            <link>https://30DaysOf.github.io/responsible-ai/twain-apr01-2024</link>
            <guid>https://30DaysOf.github.io/responsible-ai/twain-apr01-2024</guid>
            <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Last week, we saw the announcement about a new set of #ResponsibleAI tools in Azure AI, to build more secure and trustworthy generative AI applications. In this post, we'll take a tour of the tools announced with links for deeper dives]]></description>
            <content:encoded><![CDATA[
<p><img loading="lazy" src="https://media.dev.to/cdn-cgi/image/width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F202dnkp100nlo2eoe41v.png" alt="Banner" class="img_ev3q"></p>
<p><em>This post was originally publishd on dev.to as part of my This Week In AI News series</em>.</p>
<hr>
<p>We've talked about #ResponsibleAI in two contexts before - <a href="https://dev.to/azure/train-debug-ml-models-for-responsible-ai-join-the-aiskillschallenge-3pb3" target="_blank" rel="noopener noreferrer">Model Debugging</a> for predictive AI apps (MLOps), and <a href="https://dev.to/azure/fuel-your-intelligent-apps-with-azure-ai-3j4b" target="_blank" rel="noopener noreferrer">AI-Assisted Evaluation</a> for generative AI (LLMOps). Then late last week, I shared <a href="https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/" target="_blank" rel="noopener noreferrer">this exciting announcement</a> about new Responsible AI tools coming to Azure AI!</p>
<p>In today's post, I want to dig a little deeper into the announcement to learn what the tools do, why they matter, and how developers can get started using them in their generative AI application workflows.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1--the-azure-ai-platform">1 | The Azure AI Platform<a href="https://30daysof.github.io/responsible-ai/twain-apr01-2024#1--the-azure-ai-platform" class="hash-link" aria-label="Direct link to 1 | The Azure AI Platform" title="Direct link to 1 | The Azure AI Platform">​</a></h2>
<p>The <a href="https://ai.azure.com/" target="_blank" rel="noopener noreferrer">Azure AI Studio</a> provides a browser-based UI/UX for exploring the rich capabilities of the Azure AI plaform as shown below. It also supports <a href="https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/a-code-first-experience-for-building-a-copilot-with-azure-ai/ba-p/4058659" target="_blank" rel="noopener noreferrer">a code-first experience</a> for developers who prefer working with an SDK or command-line tools.</p>
<p><img loading="lazy" src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/mq4e73jqikkj2bnbnpv4.png" alt="Azure AI Studio" class="img_ev3q"></p>
<p>It streamlines your end-to-end development workflow for generative AI applications - from exploring the model catalog, to building &amp; evaluating your AI application, to deploying and monitoring the application in production. With built-in support for <em>operationalizing Responsible AI</em>, developers can go from evaluating their applications for quality, to configuring them for content safety in production.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2--new-azure-ai-tools-for-responsible-ai">2 | New Azure AI Tools for Responsible AI<a href="https://30daysof.github.io/responsible-ai/twain-apr01-2024#2--new-azure-ai-tools-for-responsible-ai" class="hash-link" aria-label="Direct link to 2 | New Azure AI Tools for Responsible AI" title="Direct link to 2 | New Azure AI Tools for Responsible AI">​</a></h2>
<p>The <a href="https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/" target="_blank" rel="noopener noreferrer">recent announcement</a> from the Responsible AI team highlights a number of new tools and capabilities that are available (or coming soon) to Azure AI, to further improve the quality and safety of generative AI application on Azure. This short video gives you a quick preview of how these tools are put to use to create safeguards for generative AI apps on Azure. In the rest of this post, we'll dive briefly into each of these tools to understand what they do, and why it matters.</p>
<iframe width="600" height="400" frameborder="0" src="https://www.youtube.com/embed/BnGmozWvsOo" title="How to Safeguard your generative AI applications in Azure AI"></iframe>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3--prompt-shields">3 | Prompt Shields<a href="https://30daysof.github.io/responsible-ai/twain-apr01-2024#3--prompt-shields" class="hash-link" aria-label="Direct link to 3 | Prompt Shields" title="Direct link to 3 | Prompt Shields">​</a></h2>
<p><img loading="lazy" src="https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/03/image-5.webp" alt="Prompt Shields" class="img_ev3q"></p>
<p>The first new capability comes in the form of <a href="https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-announces-prompt-shields-for-jailbreak-and-indirect/ba-p/4099140" target="_blank" rel="noopener noreferrer">Prompt Shields</a> that can <strong>detect and block prompt injection attacks</strong> to safeguard the <em>integrity</em> of your LLM system. These attacks work by tricking the system into harmful or unplanned behaviors, "injecting" unauthorized instructions into the default user prompt at runtime.</p>
<p>In a <strong>direct attack</strong> (jailbreak) the user is the adversary. The user prompt attempts to get the model to disregard developer-authored system prompts and training in favor of executing potentially harmful instructions. In an <strong>indirect attack</strong> (cross-domain prompt injection) the adversary is a third-party and the attack occurs via untrusted external data sources that may be embedded in the user prompt, but not authored by user or developer.</p>
<p>Prompt Shields work proactively to detect suspicious inputs in real-time and block them before they reach the LLM. This can use techniques like <a href="https://arxiv.org/abs/2403.14720" target="_blank" rel="noopener noreferrer">Spotlighting</a> that transform the input to mitigate these attacks while preserving the semantic content of the user prompt.</p>
<p>🔖 | <a href="https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-announces-prompt-shields-for-jailbreak-and-indirect/ba-p/4099140" target="_blank" rel="noopener noreferrer"><strong>Learn more in this post</strong></a>
🔖 | <a href="https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/" target="_blank" rel="noopener noreferrer"><strong>Review the main announcement</strong></a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="4--groundedness-detection">4 | Groundedness Detection<a href="https://30daysof.github.io/responsible-ai/twain-apr01-2024#4--groundedness-detection" class="hash-link" aria-label="Direct link to 4 | Groundedness Detection" title="Direct link to 4 | Groundedness Detection">​</a></h2>
<p><img loading="lazy" src="https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/03/Groundednes-Detection.webp" alt="Groundedness Detection" class="img_ev3q"></p>
<p>The second capability involves <a href="https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/detect-and-mitigate-ungrounded-model-outputs/ba-p/4099261" target="_blank" rel="noopener noreferrer"><strong>Groundedness Detection</strong></a> to combat the familiar problem of <em>"Hallucinations"</em>. Here, models fabricate a response that may look valid but is not grounded in any real data. Identifying and remediating this is critical to improve <strong>trustworthiness</strong> of generative AI responses.</p>
<p>Previously developer options included manual checks (not scalable) and chaining requests (to have an LLM evaluate if the previous response was grounded with respect to a reference document) with mixed results. The new tool uses a custom-built fine-tuned language model that detects <em>ungrounded claims</em> more accurately - giving developers multiple options to mitigate the behavior, from pre-deployment testing to post-deployment rewriting of responses.</p>
<p>🔖 | <a href="https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/detect-and-mitigate-ungrounded-model-outputs/ba-p/4099261" target="_blank" rel="noopener noreferrer"><strong>Learn more in this post</strong></a>
🔖 | <a href="https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/" target="_blank" rel="noopener noreferrer"><strong>Review the main announcement</strong></a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="5--safety-system-messages">5 | Safety System Messages<a href="https://30daysof.github.io/responsible-ai/twain-apr01-2024#5--safety-system-messages" class="hash-link" aria-label="Direct link to 5 | Safety System Messages" title="Direct link to 5 | Safety System Messages">​</a></h2>
<p>The third capability recognizes that prompt engineering is a powerful way to improve the reliability of the generative AI application, along with services like Azure AI Content Safety. Writing <strong>effective system prompts</strong> (metaprompts) can have a non-trivial impact on the quality of responses - and system messages that can "guide the optimal use of grounding data and overall behavior" are ideal.</p>
<p><img loading="lazy" src="https://learn.microsoft.com/en-us/azure/ai-services/openai/media/concepts/system-message/template.png#lightbox" alt="Safety System Messages" class="img_ev3q"></p>
<p>With the new <a href="https://learn.microsoft.com/azure/ai-services/openai/concepts/system-message" target="_blank" rel="noopener noreferrer">system message framework and template recommendations for LLMs</a>, developers now get example templates and recommendations to help them craft more effective messages. For instance, the system message framework describes four concepts (define model capabilities, define model output format, provide examples, provide behavioral guardrails) you can apply in crafting the system message. The screenshot above shows an example of how this is applied in a retail chatbot app.</p>
<p>🔖 | <a href="https://learn.microsoft.com/azure/ai-services/openai/concepts/system-message" target="_blank" rel="noopener noreferrer"><strong>Learn more from the documentation</strong></a>
🔖 | <a href="https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/" target="_blank" rel="noopener noreferrer"><strong>Review the main announcement</strong></a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="6--automated-safety-evaluations">6 | Automated Safety Evaluations<a href="https://30daysof.github.io/responsible-ai/twain-apr01-2024#6--automated-safety-evaluations" class="hash-link" aria-label="Direct link to 6 | Automated Safety Evaluations" title="Direct link to 6 | Automated Safety Evaluations">​</a></h2>
<p><img loading="lazy" src="https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/03/image-4.webp" alt="Safety Evaluations" class="img_ev3q"></p>
<p>The fourth capability recognizes that most developers lack the resources and expertise to conduct rigorous safety evaluations on their generative AI applications - which would involve curating high-quality datasets for testing, and interpreting evaluation results for effective mitigation.</p>
<p>Previously, Azure AI supported pre-built <a href="https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in?tabs=warning#generation-quality-metrics" target="_blank" rel="noopener noreferrer">generation quality metrics</a> like <em>groundedness</em>, <em>relevance</em>, <em>coherence</em> and <em>fluency</em> for AI-assisted evaluations. With the new capability, this now includes additional <a href="https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in?tabs=warning#risk-and-safety-metrics" target="_blank" rel="noopener noreferrer">risk and safety metrics</a> like <em>hateful and unfair content</em>, <em>sexual content</em>, <em>violent content</em>, <em>self-harm-related content</em>, and jailbreaks.</p>
<p><img loading="lazy" src="https://techcommunity.microsoft.com/t5/image/serverpage/image-id/565820iA1D8020BB82AD593/image-size/large?v=v2&amp;px=999" alt="Safety Evaluation Workflow" class="img_ev3q"></p>
<p>To conduct a safety evaluation on your generative AI application, you need a test dataset and some way to simulate adversarial interactions with your application so you can evaluate the resulting responses for the relevant safety metrics. The new Azure AI <strong>automated safety evaluations</strong> capability streamlines this for you in four steps:</p>
<ul>
<li>Start with targeted prompts (created from templates)</li>
<li>Use AI-assisted simulation (for adversarial interactions)</li>
<li>Create your test datasets (baseline &amp; adversarial)</li>
<li>Evaluate the test datasets (for your application)
&nbsp;
Outcomes can now be used to configure or adapt other elements of the application's risk mitigation system.</li>
</ul>
<p>🔖 | <a href="https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/introducing-ai-assisted-safety-evaluations-in-azure-ai-studio/ba-p/4098595" target="_blank" rel="noopener noreferrer"><strong>Learn more in this post</strong></a>
🔖 | <a href="https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/" target="_blank" rel="noopener noreferrer"><strong>Review the main announcement</strong></a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="7--risk--safety-monitoring">7 | Risk &amp; Safety Monitoring<a href="https://30daysof.github.io/responsible-ai/twain-apr01-2024#7--risk--safety-monitoring" class="hash-link" aria-label="Direct link to 7 | Risk &amp; Safety Monitoring" title="Direct link to 7 | Risk &amp; Safety Monitoring">​</a></h2>
<p><img loading="lazy" src="https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/03/image-6.webp" alt="Risk and Safety Monitoring" class="img_ev3q"></p>
<p>The final new capability announced was around <a href="https://learn.microsoft.com/azure/ai-services/openai/how-to/risks-safety-monitor" target="_blank" rel="noopener noreferrer">Risk &amp; Safety Monitoring in Azure Open AI</a> - adding a new Dashboard capability described as follows:</p>
<blockquote>
<p>In addition to the detection/ mitigation on harmful content in near-real time, the risks &amp; safety monitoring help get a better view of how the content filter mitigation works on real customer traffic and provide insights on potentially abusive end-users.</p>
</blockquote>
<p>To use the feature, you need an Azure OpenAI resource in a supported region, and a model deployment with a content filter configured. Once setup, simply open the <strong>Deployments</strong> tab, visit the model deployment page, and select the <strong>Risks &amp; Safety</strong> tab as shown in this figure from the announcement post below.</p>
<p><img loading="lazy" src="https://techcommunity.microsoft.com/t5/image/serverpage/image-id/565909i547F8DDB0124F9BC/image-dimensions/554x621?v=v2" alt="Filter tab" class="img_ev3q"></p>
<p>The resulting dashboard provides two kinds of insights into content filter effectiveness. The first focuses on <strong>Content Detection</strong> with visualized insights into metrics like <em>Total blocked request count and block rate</em>, <em>Blocked requests by category</em>, <em>Severity distribution by category</em> and more. The second focuses on <strong>Abusive User Detection</strong> to highlight how regularly the content filters safeguards are abused by end users and identify the severity and frequency of those occurrences.</p>
<p>🔖 | <a href="https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/introducing-risks-amp-safety-monitoring-feature-in-azure-openai/ba-p/4099218" target="_blank" rel="noopener noreferrer"><strong>Learn more in this post</strong></a>
🔖 | <a href="https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/" target="_blank" rel="noopener noreferrer"><strong>Review the main announcement</strong></a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="8--get-started-exploring">8 | Get Started Exploring<a href="https://30daysof.github.io/responsible-ai/twain-apr01-2024#8--get-started-exploring" class="hash-link" aria-label="Direct link to 8 | Get Started Exploring" title="Direct link to 8 | Get Started Exploring">​</a></h2>
<p>This was a lot - and it is still just the tip of the iceberg when it comes to actively understanding and applying responsible AI principles in practice. Want to get started exploring this topic furthere? Bookmark and revisit these three core resources:</p>
<table><thead><tr><th style="text-align:left">Resource</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left">1️⃣ <a href="https://azure.microsoft.com/en-us/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/" target="_blank" rel="noopener noreferrer">Blog Post</a></td><td style="text-align:left">Official Announcement of New Tools</td></tr><tr><td style="text-align:left">2️⃣ <a href="https://aka.ms/rai-hub/collection" target="_blank" rel="noopener noreferrer">Collection</a></td><td style="text-align:left">My Responible AI For Developers Collection</td></tr><tr><td style="text-align:left">3️⃣ <a href="https://aka.ms/ai-studio/collection" target="_blank" rel="noopener noreferrer">Collection</a></td><td style="text-align:left">My Azure AI For Developers Collection</td></tr></tbody></table>
<hr>]]></content:encoded>
            <category>responsibleai</category>
            <category>principles</category>
            <category>tools</category>
            <category>practices</category>
            <category>recipes</category>
        </item>
        <item>
            <title><![CDATA[My Responsible AI Cookbook]]></title>
            <link>https://30DaysOf.github.io/responsible-ai/cookbook-kickoff</link>
            <guid>https://30DaysOf.github.io/responsible-ai/cookbook-kickoff</guid>
            <pubDate>Sun, 31 Mar 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Responsible AI principles and practices are even more critical in the current fast-paced Generative AI ecosystem. This is the start of my learning journey to build a Responsible AI cookbook with useful recipes for generative AI developers.]]></description>
            <content:encoded><![CDATA[<p>Welcome to the kickoff of my latest project - a Responsible AI Cookbook. This is a personal learning project to explore the increasingly critical area of responsible AI - from core concepts and principles, to developer tools and workflows - with specific focus on the <strong>generative AI applications</strong> ecosystem. In this first post, I'll set the stage by defining a roadmap for what I want to learn, and explaining why I want to use the #30Days model for building a cookbook for developers.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-this-post-covers">What This Post Covers:<a href="https://30daysof.github.io/responsible-ai/cookbook-kickoff#what-this-post-covers" class="hash-link" aria-label="Direct link to What This Post Covers:" title="Direct link to What This Post Covers:">​</a></h2>
<ul>
<li>What is Responsible AI and why learn about it?</li>
<li>What is my #30DaysOf Learning Roadmap?</li>
<li>What does the Cookbook format provide?</li>
<li>How does this relate to Generative AI?</li>
<li>Resource Collection: <a href="https://aka.ms/rai-hub/collection" target="_blank" rel="noopener noreferrer"><strong>Responsible AI For Developers</strong></a></li>
</ul>
<p><img loading="lazy" alt="Deploy with Responsible AI" src="https://30daysof.github.io/responsible-ai/assets/images/000-cookbook-2bc48a0eff97d592fff1c5f4f6bb0403.png" width="1000" height="420" class="img_ev3q"></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-background">1. Background<a href="https://30daysof.github.io/responsible-ai/cookbook-kickoff#1-background" class="hash-link" aria-label="Direct link to 1. Background" title="Direct link to 1. Background">​</a></h2>
<p>As an AI Advocate at Microsoft, I've spent the last six months exploring different aspects of the Generative AI landscape - from <a href="https://dev.to/azure/prompt-engineering-fundamentals-generative-ai-for-beginners-v1-1kii" target="_blank" rel="noopener noreferrer">Prompt Engineering</a> and <a href="https://dev.to/azure/fine-tuning-fundamentals-generative-ai-for-beginners-v2-3lf9" target="_blank" rel="noopener noreferrer">Fine Tuning</a> fundamentals to <a href="https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/a-code-first-experience-for-building-a-copilot-with-azure-ai/ba-p/4058659" target="_blank" rel="noopener noreferrer">Code-first Development</a> using Azure AI Studio. But it wasn't till I started working on a <a href="https://dev.to/azure/responsible-ai-for-developers-resources-for-self-guided-learning-3lf9" target="_blank" rel="noopener noreferrer">Responsible AI Hub For Developers</a> that I realized how <em>responsible AI</em> became critical to every step of the end-to-end development workflow (LLM Ops) for generative AI solutions.</p>
<p>And while there was a lot of information about responsible AI, it was scattered across different locations - from open-source toolboxes for data scientists, to cloud-based integrations for operationalization, and community-based forums for discussing real-world case studies and best practices. I needed a way to explore this in a <em>structured</em> manner, and build myself a "cookbook" of useful recipes that I could revisit to find the right tool or process for a given task.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-what-is-responsible-ai">2. What is Responsible AI?<a href="https://30daysof.github.io/responsible-ai/cookbook-kickoff#2-what-is-responsible-ai" class="hash-link" aria-label="Direct link to 2. What is Responsible AI?" title="Direct link to 2. What is Responsible AI?">​</a></h2>
<p>Responsible AI is a <a href="https://responsibleaitoolbox.ai/" target="_blank" rel="noopener noreferrer">fundamental approach</a> to designing, developing, assessing, and deploying, AI solutions in a <strong>safe, trustworthy and ethical manner</strong>. Microsoft has an organization-wide <a href="https://www.microsoft.com/ai/responsible-ai" target="_blank" rel="noopener noreferrer">Responsible AI</a> standard and playbook anchored by six principles:</p>
<ol>
<li><strong>Fairness</strong> - AI systems should treat all people fairly.</li>
<li><strong>Reliability and Safety</strong> - AI systems should perform reliably and safely (even under unplanned conditions).</li>
<li><strong>Privacy and Security</strong> - AI systems should be secure and respect privacy (think system security and user data privacy)</li>
<li><strong>Inclusiveness</strong> - AI systems should empower everyone and engage people (think about accessible experiences)</li>
<li><strong>Transparency</strong> - AI systems should be understandable. (think about interpretability and explainability for decisions)</li>
<li>Accountability - People should be accountable for AI systems (think about oversight and guardrails for ensuring compliance)</li>
</ol>
<p><img loading="lazy" alt="Principles of Responsible AI" src="https://30daysof.github.io/responsible-ai/assets/images/000-responsibleai-principles-8175eee34dc5e0e1a6f6445d5f2e260a.png" width="1892" height="1064" class="img_ev3q"></p>
<p>My perspective here is that we need to <em>shift left</em> in the application lifecycle when it comes to detecting and mitigating or eliminating potential harms that can adversely impact responsible AI usage. And that means raising <strong>developer awareness</strong> of these issues and documenting <strong>actionable recipes</strong> that can be integrated into their design thinking and developer workflows.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-my-30days-roadmap">3. My #30Days Roadmap<a href="https://30daysof.github.io/responsible-ai/cookbook-kickoff#3-my-30days-roadmap" class="hash-link" aria-label="Direct link to 3. My #30Days Roadmap" title="Direct link to 3. My #30Days Roadmap">​</a></h2>
<p>My #30DaysOf platform and process started off as a way to create <em>themed month</em> content to give learners a deep-dive into a given topic in the span of four weeks. However, I want to repurpose the format for <em>self-paced learning</em> by keeping the same structure (roadmap) but without the time pressure for completion (30 days). The <em>Cookbook</em> format seemed to work best:</p>
<ol>
<li>The #30Days site becomes a developer cookbook for a given topic.</li>
<li>The Roadmap is the table-of-contents, organized as top-level collections.</li>
<li>The Collection is a set of recipes focused on a particular sub-topic.</li>
<li>Each Recipe has an explainer (what is it) and an exercise (how do I use it).</li>
</ol>
<p>My immediate goal is to build this cookbook <strong>for my own use</strong> - all mistakes and misconceptions are mine and the content is intended for learning purposes only and not for production use. With that in mind, my first steps are to find all relevant learning resources and organize them into a hierarchical roadmap (tree of nodes) with the following top-level nodes:</p>
<ul>
<li>Core Concepts - understand key terminology and building blocks</li>
<li>Developer Tools - explore key tools and workflows for developers</li>
<li>Generative AI - understand Responsible AI usage for LLM Ops</li>
<li>Advanced Topics - covers research, best practices &amp; new projects.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="4-generative-ai-and-llm-ops">4. Generative AI and LLM Ops<a href="https://30daysof.github.io/responsible-ai/cookbook-kickoff#4-generative-ai-and-llm-ops" class="hash-link" aria-label="Direct link to 4. Generative AI and LLM Ops" title="Direct link to 4. Generative AI and LLM Ops">​</a></h2>
<p>Traditional AI applications focused on training models with domain-specific datasets, and deploying <em>predictive AI</em> endpoints that applications could use for decision-making. Modern <em>generative AI</em> applications focus on pre-trained "large language models" trained on massive internet-scale data, with endpoints that applications can use for generating original content (text, code, images, audio, video etc.).</p>
<p>This unlocks a whole new range of user experiences driven by <em>natural language</em> interactions (chat), driving a paradigm shift from MLOps to LLMOps. In my day job as an AI Advocate at Microsoft, I focus on how we can use the <a href="https://ai.azure.com/" target="_blank" rel="noopener noreferrer">Azure AI platform</a> to streamline the end-to-end development of Generative AI applications - and use this figure to explain what the LLM Ops (or end-to-end application lifecycle) would involve.</p>
<p><img loading="lazy" src="https://techcommunity.microsoft.com/t5/image/serverpage/image-id/552316i571E7F652BBB7F1C/image-size/large?v=v2&amp;px=999" alt="LLM Ops Workflow" class="img_ev3q"></p>
<p>When we think about Responsible AI in the context of this workflow, we have to <a href="https://learn.microsoft.com/training/modules/responsible-generative-ai/2-plan-responsible-ai" target="_blank" rel="noopener noreferrer">plan a responsible generative AI solution</a> with 4 steps:</p>
<ul>
<li><em>Identify</em> potential harms relevant to your solution</li>
<li><em>Evaluate</em> presence of harms in generated outputs</li>
<li><em>Mitigate</em> harms at multiple levels to minimize impact</li>
<li><em>Operate</em> solution responsibly with an ops readiness plan.</li>
</ul>
<p>With the Azure AI platform, this is captured in the <a href="https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-approach-gen-ai" target="_blank" rel="noopener noreferrer">approach to generative AI evaluations</a>, the <a href="https://learn.microsoft.com/azure/ai-services/responsible-use-of-ai-overview" target="_blank" rel="noopener noreferrer">responsible AI guidelines for Azure AI services</a> and the <a href="https://learn.microsoft.com/azure/ai-studio/concepts/content-filtering" target="_blank" rel="noopener noreferrer">content filtering system</a>for deployments.</p>
<p><img loading="lazy" src="https://learn.microsoft.com/en-us/azure/ai-studio/media/evaluations/evaluation-monitor-flow.png" alt="Evaluation" class="img_ev3q"></p>
<p>The figure above gives us a high-level sense for where these mechanisms fit against the three stages in the LLM Ops diagram above. And just last week, the team announced <a href="https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/" target="_blank" rel="noopener noreferrer">new tools for building secure and trustworthy generative AI applications</a> that enhance the workflow even more to tackle challenges like prompt injection attacks, vulnerability to jailbreaking, detection of "hallucinations" and more.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="5-next-steps">5. Next Steps<a href="https://30daysof.github.io/responsible-ai/cookbook-kickoff#5-next-steps" class="hash-link" aria-label="Direct link to 5. Next Steps" title="Direct link to 5. Next Steps">​</a></h2>
<p>My next step is to build a roadmap (table of contents with links to relevant resources) to help me plan out my learning journey. Stay tuned!</p>]]></content:encoded>
            <category>responsibleai</category>
            <category>principles</category>
            <category>tools</category>
            <category>practices</category>
            <category>recipes</category>
        </item>
    </channel>
</rss>