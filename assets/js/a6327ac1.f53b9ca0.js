"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[6553],{6334:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var n=i(5893),s=i(1151);const o={slug:"twain-apr01-2024",title:"Responsible AI On Azure",description:"Last week, we saw the announcement about a new set of #ResponsibleAI tools in Azure AI, to build more secure and trustworthy generative AI applications. In this post, we'll take a tour of the tools announced with links for deeper dives",authors:["nitya"],keywords:["azure","ai-studio","responsible-ai","generative-ai","rai-dashboard","rai-evaluation","rai-content-safety","rai-cookbook"],tags:["responsibleai","principles","tools","practices","recipes"]},a=void 0,r={permalink:"/responsible-ai/twain-apr01-2024",source:"@site/blog/2024-04-01/index.md",title:"Responsible AI On Azure",description:"Last week, we saw the announcement about a new set of #ResponsibleAI tools in Azure AI, to build more secure and trustworthy generative AI applications. In this post, we'll take a tour of the tools announced with links for deeper dives",date:"2024-04-01T00:00:00.000Z",formattedDate:"April 1, 2024",tags:[{label:"responsibleai",permalink:"/responsible-ai/tags/responsibleai"},{label:"principles",permalink:"/responsible-ai/tags/principles"},{label:"tools",permalink:"/responsible-ai/tags/tools"},{label:"practices",permalink:"/responsible-ai/tags/practices"},{label:"recipes",permalink:"/responsible-ai/tags/recipes"}],readingTime:6.43,hasTruncateMarker:!1,authors:[{name:"Nitya Narasimhan",title:"PhD & Polyglot, AI @Microsoft",url:"https://github.com/nitya",imageURL:"https://github.com/nitya.png",key:"nitya"}],frontMatter:{slug:"twain-apr01-2024",title:"Responsible AI On Azure",description:"Last week, we saw the announcement about a new set of #ResponsibleAI tools in Azure AI, to build more secure and trustworthy generative AI applications. In this post, we'll take a tour of the tools announced with links for deeper dives",authors:["nitya"],keywords:["azure","ai-studio","responsible-ai","generative-ai","rai-dashboard","rai-evaluation","rai-content-safety","rai-cookbook"],tags:["responsibleai","principles","tools","practices","recipes"]},unlisted:!1,nextItem:{title:"My Responsible AI Cookbook",permalink:"/responsible-ai/cookbook-kickoff"}},l={authorsImageUrls:[void 0]},c=[{value:"1 | The Azure AI Platform",id:"1--the-azure-ai-platform",level:2},{value:"2 | New Azure AI Tools for Responsible AI",id:"2--new-azure-ai-tools-for-responsible-ai",level:2},{value:"3 | Prompt Shields",id:"3--prompt-shields",level:2},{value:"4 | Groundedness Detection",id:"4--groundedness-detection",level:2},{value:"5 | Safety System Messages",id:"5--safety-system-messages",level:2},{value:"6 | Automated Safety Evaluations",id:"6--automated-safety-evaluations",level:2},{value:"7 | Risk &amp; Safety Monitoring",id:"7--risk--safety-monitoring",level:2},{value:"8 | Get Started Exploring",id:"8--get-started-exploring",level:2}];function d(e){const t={a:"a",blockquote:"blockquote",em:"em",h2:"h2",hr:"hr",img:"img",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.a)(),...e.components},{Head:i}=t;return i||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Head",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(i,{children:(0,n.jsx)("link",{rel:"canonical",href:"e-ai-tools-to-help-operationalize-responsible-ai-for-generative-ai-apps-11ig"})}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:"https://media.dev.to/cdn-cgi/image/width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F202dnkp100nlo2eoe41v.png",alt:"Banner"})}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.em,{children:"This post was originally publishd on dev.to as part of my This Week In AI News series"}),"."]}),"\n",(0,n.jsx)(t.hr,{}),"\n",(0,n.jsxs)(t.p,{children:["We've talked about #ResponsibleAI in two contexts before - ",(0,n.jsx)(t.a,{href:"https://dev.to/azure/train-debug-ml-models-for-responsible-ai-join-the-aiskillschallenge-3pb3",children:"Model Debugging"})," for predictive AI apps (MLOps), and ",(0,n.jsx)(t.a,{href:"https://dev.to/azure/fuel-your-intelligent-apps-with-azure-ai-3j4b",children:"AI-Assisted Evaluation"})," for generative AI (LLMOps). Then late last week, I shared ",(0,n.jsx)(t.a,{href:"https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/",children:"this exciting announcement"})," about new Responsible AI tools coming to Azure AI!"]}),"\n",(0,n.jsx)(t.p,{children:"In today's post, I want to dig a little deeper into the announcement to learn what the tools do, why they matter, and how developers can get started using them in their generative AI application workflows."}),"\n",(0,n.jsx)(t.hr,{}),"\n",(0,n.jsx)(t.h2,{id:"1--the-azure-ai-platform",children:"1 | The Azure AI Platform"}),"\n",(0,n.jsxs)(t.p,{children:["The ",(0,n.jsx)(t.a,{href:"https://ai.azure.com",children:"Azure AI Studio"})," provides a browser-based UI/UX for exploring the rich capabilities of the Azure AI plaform as shown below. It also supports ",(0,n.jsx)(t.a,{href:"https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/a-code-first-experience-for-building-a-copilot-with-azure-ai/ba-p/4058659",children:"a code-first experience"})," for developers who prefer working with an SDK or command-line tools."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/mq4e73jqikkj2bnbnpv4.png",alt:"Azure AI Studio"})}),"\n",(0,n.jsxs)(t.p,{children:["It streamlines your end-to-end development workflow for generative AI applications - from exploring the model catalog, to building & evaluating your AI application, to deploying and monitoring the application in production. With built-in support for ",(0,n.jsx)(t.em,{children:"operationalizing Responsible AI"}),", developers can go from evaluating their applications for quality, to configuring them for content safety in production."]}),"\n",(0,n.jsx)(t.hr,{}),"\n",(0,n.jsx)(t.h2,{id:"2--new-azure-ai-tools-for-responsible-ai",children:"2 | New Azure AI Tools for Responsible AI"}),"\n",(0,n.jsxs)(t.p,{children:["The ",(0,n.jsx)(t.a,{href:"https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/",children:"recent announcement"})," from the Responsible AI team highlights a number of new tools and capabilities that are available (or coming soon) to Azure AI, to further improve the quality and safety of generative AI application on Azure. This short video gives you a quick preview of how these tools are put to use to create safeguards for generative AI apps on Azure. In the rest of this post, we'll dive briefly into each of these tools to understand what they do, and why it matters."]}),"\n",(0,n.jsx)("iframe",{width:"600",height:"400",frameborder:"0",src:"https://www.youtube.com/embed/BnGmozWvsOo",title:"How to Safeguard your generative AI applications in Azure AI",frameborder:"0"}),"\n",(0,n.jsx)(t.hr,{}),"\n",(0,n.jsx)(t.h2,{id:"3--prompt-shields",children:"3 | Prompt Shields"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:"https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/03/image-5.webp",alt:"Prompt Shields"})}),"\n",(0,n.jsxs)(t.p,{children:["The first new capability comes in the form of ",(0,n.jsx)(t.a,{href:"https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-announces-prompt-shields-for-jailbreak-and-indirect/ba-p/4099140",children:"Prompt Shields"})," that can ",(0,n.jsx)(t.strong,{children:"detect and block prompt injection attacks"})," to safeguard the ",(0,n.jsx)(t.em,{children:"integrity"}),' of your LLM system. These attacks work by tricking the system into harmful or unplanned behaviors, "injecting" unauthorized instructions into the default user prompt at runtime.']}),"\n",(0,n.jsxs)(t.p,{children:["In a ",(0,n.jsx)(t.strong,{children:"direct attack"})," (jailbreak) the user is the adversary. The user prompt attempts to get the model to disregard developer-authored system prompts and training in favor of executing potentially harmful instructions. In an ",(0,n.jsx)(t.strong,{children:"indirect attack"})," (cross-domain prompt injection) the adversary is a third-party and the attack occurs via untrusted external data sources that may be embedded in the user prompt, but not authored by user or developer."]}),"\n",(0,n.jsxs)(t.p,{children:["Prompt Shields work proactively to detect suspicious inputs in real-time and block them before they reach the LLM. This can use techniques like ",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2403.14720",children:"Spotlighting"})," that transform the input to mitigate these attacks while preserving the semantic content of the user prompt."]}),"\n",(0,n.jsxs)(t.p,{children:["\ud83d\udd16 | ",(0,n.jsx)(t.a,{href:"https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-announces-prompt-shields-for-jailbreak-and-indirect/ba-p/4099140",children:(0,n.jsx)(t.strong,{children:"Learn more in this post"})}),"\n\ud83d\udd16 | ",(0,n.jsx)(t.a,{href:"https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/",children:(0,n.jsx)(t.strong,{children:"Review the main announcement"})})]}),"\n",(0,n.jsx)(t.hr,{}),"\n",(0,n.jsx)(t.h2,{id:"4--groundedness-detection",children:"4 | Groundedness Detection"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:"https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/03/Groundednes-Detection.webp",alt:"Groundedness Detection"})}),"\n",(0,n.jsxs)(t.p,{children:["The second capability involves ",(0,n.jsx)(t.a,{href:"https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/detect-and-mitigate-ungrounded-model-outputs/ba-p/4099261",children:(0,n.jsx)(t.strong,{children:"Groundedness Detection"})})," to combat the familiar problem of ",(0,n.jsx)(t.em,{children:'"Hallucinations"'}),". Here, models fabricate a response that may look valid but is not grounded in any real data. Identifying and remediating this is critical to improve ",(0,n.jsx)(t.strong,{children:"trustworthiness"})," of generative AI responses."]}),"\n",(0,n.jsxs)(t.p,{children:["Previously developer options included manual checks (not scalable) and chaining requests (to have an LLM evaluate if the previous response was grounded with respect to a reference document) with mixed results. The new tool uses a custom-built fine-tuned language model that detects ",(0,n.jsx)(t.em,{children:"ungrounded claims"})," more accurately - giving developers multiple options to mitigate the behavior, from pre-deployment testing to post-deployment rewriting of responses."]}),"\n",(0,n.jsxs)(t.p,{children:["\ud83d\udd16 | ",(0,n.jsx)(t.a,{href:"https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/detect-and-mitigate-ungrounded-model-outputs/ba-p/4099261",children:(0,n.jsx)(t.strong,{children:"Learn more in this post"})}),"\n\ud83d\udd16 | ",(0,n.jsx)(t.a,{href:"https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/",children:(0,n.jsx)(t.strong,{children:"Review the main announcement"})})]}),"\n",(0,n.jsx)(t.hr,{}),"\n",(0,n.jsx)(t.h2,{id:"5--safety-system-messages",children:"5 | Safety System Messages"}),"\n",(0,n.jsxs)(t.p,{children:["The third capability recognizes that prompt engineering is a powerful way to improve the reliability of the generative AI application, along with services like Azure AI Content Safety. Writing ",(0,n.jsx)(t.strong,{children:"effective system prompts"}),' (metaprompts) can have a non-trivial impact on the quality of responses - and system messages that can "guide the optimal use of grounding data and overall behavior" are ideal.']}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:"https://learn.microsoft.com/en-us/azure/ai-services/openai/media/concepts/system-message/template.png#lightbox",alt:"Safety System Messages"})}),"\n",(0,n.jsxs)(t.p,{children:["With the new ",(0,n.jsx)(t.a,{href:"https://learn.microsoft.com/azure/ai-services/openai/concepts/system-message",children:"system message framework and template recommendations for LLMs"}),", developers now get example templates and recommendations to help them craft more effective messages. For instance, the system message framework describes four concepts (define model capabilities, define model output format, provide examples, provide behavioral guardrails) you can apply in crafting the system message. The screenshot above shows an example of how this is applied in a retail chatbot app."]}),"\n",(0,n.jsxs)(t.p,{children:["\ud83d\udd16 | ",(0,n.jsx)(t.a,{href:"https://learn.microsoft.com/azure/ai-services/openai/concepts/system-message",children:(0,n.jsx)(t.strong,{children:"Learn more from the documentation"})}),"\n\ud83d\udd16 | ",(0,n.jsx)(t.a,{href:"https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/",children:(0,n.jsx)(t.strong,{children:"Review the main announcement"})})]}),"\n",(0,n.jsx)(t.hr,{}),"\n",(0,n.jsx)(t.h2,{id:"6--automated-safety-evaluations",children:"6 | Automated Safety Evaluations"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:"https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/03/image-4.webp",alt:"Safety Evaluations"})}),"\n",(0,n.jsx)(t.p,{children:"The fourth capability recognizes that most developers lack the resources and expertise to conduct rigorous safety evaluations on their generative AI applications - which would involve curating high-quality datasets for testing, and interpreting evaluation results for effective mitigation."}),"\n",(0,n.jsxs)(t.p,{children:["Previously, Azure AI supported pre-built ",(0,n.jsx)(t.a,{href:"https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in?tabs=warning#generation-quality-metrics",children:"generation quality metrics"})," like ",(0,n.jsx)(t.em,{children:"groundedness"}),", ",(0,n.jsx)(t.em,{children:"relevance"}),", ",(0,n.jsx)(t.em,{children:"coherence"})," and ",(0,n.jsx)(t.em,{children:"fluency"})," for AI-assisted evaluations. With the new capability, this now includes additional ",(0,n.jsx)(t.a,{href:"https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in?tabs=warning#risk-and-safety-metrics",children:"risk and safety metrics"})," like ",(0,n.jsx)(t.em,{children:"hateful and unfair content"}),", ",(0,n.jsx)(t.em,{children:"sexual content"}),", ",(0,n.jsx)(t.em,{children:"violent content"}),", ",(0,n.jsx)(t.em,{children:"self-harm-related content"}),", and jailbreaks."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:"https://techcommunity.microsoft.com/t5/image/serverpage/image-id/565820iA1D8020BB82AD593/image-size/large?v=v2&px=999",alt:"Safety Evaluation Workflow"})}),"\n",(0,n.jsxs)(t.p,{children:["To conduct a safety evaluation on your generative AI application, you need a test dataset and some way to simulate adversarial interactions with your application so you can evaluate the resulting responses for the relevant safety metrics. The new Azure AI ",(0,n.jsx)(t.strong,{children:"automated safety evaluations"})," capability streamlines this for you in four steps:"]}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"Start with targeted prompts (created from templates)"}),"\n",(0,n.jsx)(t.li,{children:"Use AI-assisted simulation (for adversarial interactions)"}),"\n",(0,n.jsx)(t.li,{children:"Create your test datasets (baseline & adversarial)"}),"\n",(0,n.jsx)(t.li,{children:"Evaluate the test datasets (for your application)\n\xa0\nOutcomes can now be used to configure or adapt other elements of the application's risk mitigation system."}),"\n"]}),"\n",(0,n.jsxs)(t.p,{children:["\ud83d\udd16 | ",(0,n.jsx)(t.a,{href:"https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/introducing-ai-assisted-safety-evaluations-in-azure-ai-studio/ba-p/4098595",children:(0,n.jsx)(t.strong,{children:"Learn more in this post"})}),"\n\ud83d\udd16 | ",(0,n.jsx)(t.a,{href:"https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/",children:(0,n.jsx)(t.strong,{children:"Review the main announcement"})})]}),"\n",(0,n.jsx)(t.hr,{}),"\n",(0,n.jsx)(t.h2,{id:"7--risk--safety-monitoring",children:"7 | Risk & Safety Monitoring"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:"https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/03/image-6.webp",alt:"Risk and Safety Monitoring"})}),"\n",(0,n.jsxs)(t.p,{children:["The final new capability announced was around ",(0,n.jsx)(t.a,{href:"https://learn.microsoft.com/azure/ai-services/openai/how-to/risks-safety-monitor",children:"Risk & Safety Monitoring in Azure Open AI"})," - adding a new Dashboard capability described as follows:"]}),"\n",(0,n.jsxs)(t.blockquote,{children:["\n",(0,n.jsx)(t.p,{children:"In addition to the detection/ mitigation on harmful content in near-real time, the risks & safety monitoring help get a better view of how the content filter mitigation works on real customer traffic and provide insights on potentially abusive end-users."}),"\n"]}),"\n",(0,n.jsxs)(t.p,{children:["To use the feature, you need an Azure OpenAI resource in a supported region, and a model deployment with a content filter configured. Once setup, simply open the ",(0,n.jsx)(t.strong,{children:"Deployments"})," tab, visit the model deployment page, and select the ",(0,n.jsx)(t.strong,{children:"Risks & Safety"})," tab as shown in this figure from the announcement post below."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:"https://techcommunity.microsoft.com/t5/image/serverpage/image-id/565909i547F8DDB0124F9BC/image-dimensions/554x621?v=v2",alt:"Filter tab"})}),"\n",(0,n.jsxs)(t.p,{children:["The resulting dashboard provides two kinds of insights into content filter effectiveness. The first focuses on ",(0,n.jsx)(t.strong,{children:"Content Detection"})," with visualized insights into metrics like ",(0,n.jsx)(t.em,{children:"Total blocked request count and block rate"}),", ",(0,n.jsx)(t.em,{children:"Blocked requests by category"}),", ",(0,n.jsx)(t.em,{children:"Severity distribution by category"})," and more. The second focuses on ",(0,n.jsx)(t.strong,{children:"Abusive User Detection"})," to highlight how regularly the content filters safeguards are abused by end users and identify the severity and frequency of those occurrences."]}),"\n",(0,n.jsxs)(t.p,{children:["\ud83d\udd16 | ",(0,n.jsx)(t.a,{href:"https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/introducing-risks-amp-safety-monitoring-feature-in-azure-openai/ba-p/4099218",children:(0,n.jsx)(t.strong,{children:"Learn more in this post"})}),"\n\ud83d\udd16 | ",(0,n.jsx)(t.a,{href:"https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/",children:(0,n.jsx)(t.strong,{children:"Review the main announcement"})})]}),"\n",(0,n.jsx)(t.hr,{}),"\n",(0,n.jsx)(t.h2,{id:"8--get-started-exploring",children:"8 | Get Started Exploring"}),"\n",(0,n.jsx)(t.p,{children:"This was a lot - and it is still just the tip of the iceberg when it comes to actively understanding and applying responsible AI principles in practice. Want to get started exploring this topic furthere? Bookmark and revisit these three core resources:"}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{style:{textAlign:"left"},children:"Resource"}),(0,n.jsx)(t.th,{style:{textAlign:"left"},children:"Description"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsxs)(t.td,{style:{textAlign:"left"},children:["1\ufe0f\u20e3 ",(0,n.jsx)(t.a,{href:"https://azure.microsoft.com/en-us/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/",children:"Blog Post"})]}),(0,n.jsx)(t.td,{style:{textAlign:"left"},children:"Official Announcement of New Tools"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsxs)(t.td,{style:{textAlign:"left"},children:["2\ufe0f\u20e3 ",(0,n.jsx)(t.a,{href:"https://aka.ms/rai-hub/collection",children:"Collection"})]}),(0,n.jsx)(t.td,{style:{textAlign:"left"},children:"My Responible AI For Developers Collection"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsxs)(t.td,{style:{textAlign:"left"},children:["3\ufe0f\u20e3 ",(0,n.jsx)(t.a,{href:"https://aka.ms/ai-studio/collection",children:"Collection"})]}),(0,n.jsx)(t.td,{style:{textAlign:"left"},children:"My Azure AI For Developers Collection"})]})]})]}),"\n",(0,n.jsx)(t.hr,{})]})}function h(e={}){const{wrapper:t}={...(0,s.a)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},1151:(e,t,i)=>{i.d(t,{Z:()=>r,a:()=>a});var n=i(7294);const s={},o=n.createContext(s);function a(e){const t=n.useContext(o);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),n.createElement(o.Provider,{value:t},e.children)}}}]);