"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[753],{3652:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"twain-apr01-2024","metadata":{"permalink":"/responsible-ai/twain-apr01-2024","source":"@site/blog/2024-04-01/index.md","title":"Responsible AI On Azure","description":"Last week, we saw the announcement about a new set of #ResponsibleAI tools in Azure AI, to build more secure and trustworthy generative AI applications. In this post, we\'ll take a tour of the tools announced with links for deeper dives","date":"2024-04-01T00:00:00.000Z","formattedDate":"April 1, 2024","tags":[{"label":"responsibleai","permalink":"/responsible-ai/tags/responsibleai"},{"label":"principles","permalink":"/responsible-ai/tags/principles"},{"label":"tools","permalink":"/responsible-ai/tags/tools"},{"label":"practices","permalink":"/responsible-ai/tags/practices"},{"label":"recipes","permalink":"/responsible-ai/tags/recipes"}],"readingTime":6.43,"hasTruncateMarker":false,"authors":[{"name":"Nitya Narasimhan","title":"PhD & Polyglot, AI @Microsoft","url":"https://github.com/nitya","imageURL":"https://github.com/nitya.png","key":"nitya"}],"frontMatter":{"slug":"twain-apr01-2024","title":"Responsible AI On Azure","description":"Last week, we saw the announcement about a new set of #ResponsibleAI tools in Azure AI, to build more secure and trustworthy generative AI applications. In this post, we\'ll take a tour of the tools announced with links for deeper dives","authors":["nitya"],"keywords":["azure","ai-studio","responsible-ai","generative-ai","rai-dashboard","rai-evaluation","rai-content-safety","rai-cookbook"],"tags":["responsibleai","principles","tools","practices","recipes"]},"unlisted":false,"nextItem":{"title":"My Responsible AI Cookbook","permalink":"/responsible-ai/cookbook-kickoff"}},"content":"<head>\\n  <link rel=\\"canonical\\" href=\\"e-ai-tools-to-help-operationalize-responsible-ai-for-generative-ai-apps-11ig\\" />\\n</head>\\n\\n![Banner](https://media.dev.to/cdn-cgi/image/width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F202dnkp100nlo2eoe41v.png)\\n\\n_This post was originally publishd on dev.to as part of my This Week In AI News series_.\\n\\n---\\n\\nWe\'ve talked about #ResponsibleAI in two contexts before - [Model Debugging](https://dev.to/azure/train-debug-ml-models-for-responsible-ai-join-the-aiskillschallenge-3pb3) for predictive AI apps (MLOps), and [AI-Assisted Evaluation](https://dev.to/azure/fuel-your-intelligent-apps-with-azure-ai-3j4b) for generative AI (LLMOps). Then late last week, I shared [this exciting announcement](https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/) about new Responsible AI tools coming to Azure AI!\\n\\n\\nIn today\'s post, I want to dig a little deeper into the announcement to learn what the tools do, why they matter, and how developers can get started using them in their generative AI application workflows.\\n\\n---\\n\\n## 1 | The Azure AI Platform\\n\\nThe [Azure AI Studio](https://ai.azure.com) provides a browser-based UI/UX for exploring the rich capabilities of the Azure AI plaform as shown below. It also supports [a code-first experience](https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/a-code-first-experience-for-building-a-copilot-with-azure-ai/ba-p/4058659) for developers who prefer working with an SDK or command-line tools.\\n\\n![Azure AI Studio](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/mq4e73jqikkj2bnbnpv4.png)\\n\\nIt streamlines your end-to-end development workflow for generative AI applications - from exploring the model catalog, to building & evaluating your AI application, to deploying and monitoring the application in production. With built-in support for _operationalizing Responsible AI_, developers can go from evaluating their applications for quality, to configuring them for content safety in production.\\n\\n---\\n\\n## 2 | New Azure AI Tools for Responsible AI\\n\\nThe [recent announcement](https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/) from the Responsible AI team highlights a number of new tools and capabilities that are available (or coming soon) to Azure AI, to further improve the quality and safety of generative AI application on Azure. This short video gives you a quick preview of how these tools are put to use to create safeguards for generative AI apps on Azure. In the rest of this post, we\'ll dive briefly into each of these tools to understand what they do, and why it matters.\\n\\n<iframe width=\\"600\\" height=\\"400\\" frameborder=\\"0\\" src=\\"https://www.youtube.com/embed/BnGmozWvsOo\\" title=\\"How to Safeguard your generative AI applications in Azure AI\\" frameborder=\\"0\\"></iframe>\\n\\n---\\n\\n## 3 | Prompt Shields\\n\\n![Prompt Shields](https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/03/image-5.webp) \\n\\nThe first new capability comes in the form of [Prompt Shields](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-announces-prompt-shields-for-jailbreak-and-indirect/ba-p/4099140) that can **detect and block prompt injection attacks** to safeguard the _integrity_ of your LLM system. These attacks work by tricking the system into harmful or unplanned behaviors, \\"injecting\\" unauthorized instructions into the default user prompt at runtime.\\n\\nIn a **direct attack** (jailbreak) the user is the adversary. The user prompt attempts to get the model to disregard developer-authored system prompts and training in favor of executing potentially harmful instructions. In an **indirect attack** (cross-domain prompt injection) the adversary is a third-party and the attack occurs via untrusted external data sources that may be embedded in the user prompt, but not authored by user or developer.\\n\\nPrompt Shields work proactively to detect suspicious inputs in real-time and block them before they reach the LLM. This can use techniques like [Spotlighting](https://arxiv.org/abs/2403.14720) that transform the input to mitigate these attacks while preserving the semantic content of the user prompt.\\n\\n\ud83d\udd16 | [**Learn more in this post**](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-announces-prompt-shields-for-jailbreak-and-indirect/ba-p/4099140)\\n\ud83d\udd16 | [**Review the main announcement**](https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/)\\n\\n\\n---\\n\\n## 4 | Groundedness Detection\\n\\n![Groundedness Detection](https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/03/Groundednes-Detection.webp) \\n\\nThe second capability involves [**Groundedness Detection**](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/detect-and-mitigate-ungrounded-model-outputs/ba-p/4099261) to combat the familiar problem of _\\"Hallucinations\\"_. Here, models fabricate a response that may look valid but is not grounded in any real data. Identifying and remediating this is critical to improve **trustworthiness** of generative AI responses.\\n\\nPreviously developer options included manual checks (not scalable) and chaining requests (to have an LLM evaluate if the previous response was grounded with respect to a reference document) with mixed results. The new tool uses a custom-built fine-tuned language model that detects _ungrounded claims_ more accurately - giving developers multiple options to mitigate the behavior, from pre-deployment testing to post-deployment rewriting of responses.\\n\\n\ud83d\udd16 | [**Learn more in this post**](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/detect-and-mitigate-ungrounded-model-outputs/ba-p/4099261)\\n\ud83d\udd16 | [**Review the main announcement**](https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/)\\n\\n---\\n\\n## 5 | Safety System Messages\\n\\nThe third capability recognizes that prompt engineering is a powerful way to improve the reliability of the generative AI application, along with services like Azure AI Content Safety. Writing **effective system prompts** (metaprompts) can have a non-trivial impact on the quality of responses - and system messages that can \\"guide the optimal use of grounding data and overall behavior\\" are ideal.\\n\\n![Safety System Messages](https://learn.microsoft.com/en-us/azure/ai-services/openai/media/concepts/system-message/template.png#lightbox)\\n\\nWith the new [system message framework and template recommendations for LLMs](https://learn.microsoft.com/azure/ai-services/openai/concepts/system-message), developers now get example templates and recommendations to help them craft more effective messages. For instance, the system message framework describes four concepts (define model capabilities, define model output format, provide examples, provide behavioral guardrails) you can apply in crafting the system message. The screenshot above shows an example of how this is applied in a retail chatbot app.\\n\\n\ud83d\udd16 | [**Learn more from the documentation**](https://learn.microsoft.com/azure/ai-services/openai/concepts/system-message)\\n\ud83d\udd16 | [**Review the main announcement**](https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/)\\n\\n\\n---\\n\\n## 6 | Automated Safety Evaluations\\n\\n![Safety Evaluations](https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/03/image-4.webp)\\n\\nThe fourth capability recognizes that most developers lack the resources and expertise to conduct rigorous safety evaluations on their generative AI applications - which would involve curating high-quality datasets for testing, and interpreting evaluation results for effective mitigation.\\n\\nPreviously, Azure AI supported pre-built [generation quality metrics](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in?tabs=warning#generation-quality-metrics) like _groundedness_, _relevance_, _coherence_ and _fluency_ for AI-assisted evaluations. With the new capability, this now includes additional [risk and safety metrics](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in?tabs=warning#risk-and-safety-metrics) like _hateful and unfair content_, _sexual content_, _violent content_, _self-harm-related content_, and jailbreaks.\\n\\n![Safety Evaluation Workflow](https://techcommunity.microsoft.com/t5/image/serverpage/image-id/565820iA1D8020BB82AD593/image-size/large?v=v2&px=999)\\n\\nTo conduct a safety evaluation on your generative AI application, you need a test dataset and some way to simulate adversarial interactions with your application so you can evaluate the resulting responses for the relevant safety metrics. The new Azure AI **automated safety evaluations** capability streamlines this for you in four steps: \\n - Start with targeted prompts (created from templates)\\n - Use AI-assisted simulation (for adversarial interactions)\\n - Create your test datasets (baseline & adversarial)\\n - Evaluate the test datasets (for your application)\\n\xa0\\nOutcomes can now be used to configure or adapt other elements of the application\'s risk mitigation system.\\n\\n\ud83d\udd16 | [**Learn more in this post**](https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/introducing-ai-assisted-safety-evaluations-in-azure-ai-studio/ba-p/4098595)\\n\ud83d\udd16 | [**Review the main announcement**](https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/)\\n\\n---\\n\\n## 7 | Risk & Safety Monitoring\\n\\n![Risk and Safety Monitoring](https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/03/image-6.webp)\\n\\nThe final new capability announced was around [Risk & Safety Monitoring in Azure Open AI](https://learn.microsoft.com/azure/ai-services/openai/how-to/risks-safety-monitor) - adding a new Dashboard capability described as follows:\\n\\n> In addition to the detection/ mitigation on harmful content in near-real time, the risks & safety monitoring help get a better view of how the content filter mitigation works on real customer traffic and provide insights on potentially abusive end-users.\\n\\nTo use the feature, you need an Azure OpenAI resource in a supported region, and a model deployment with a content filter configured. Once setup, simply open the **Deployments** tab, visit the model deployment page, and select the **Risks & Safety** tab as shown in this figure from the announcement post below.\\n\\n![Filter tab](https://techcommunity.microsoft.com/t5/image/serverpage/image-id/565909i547F8DDB0124F9BC/image-dimensions/554x621?v=v2)\\n\\nThe resulting dashboard provides two kinds of insights into content filter effectiveness. The first focuses on **Content Detection** with visualized insights into metrics like _Total blocked request count and block rate_, _Blocked requests by category_, _Severity distribution by category_ and more. The second focuses on **Abusive User Detection** to highlight how regularly the content filters safeguards are abused by end users and identify the severity and frequency of those occurrences.\\n\\n\ud83d\udd16 | [**Learn more in this post**](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/introducing-risks-amp-safety-monitoring-feature-in-azure-openai/ba-p/4099218)\\n\ud83d\udd16 | [**Review the main announcement**](https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/)\\n\\n---\\n\\n## 8 | Get Started Exploring\\n\\nThis was a lot - and it is still just the tip of the iceberg when it comes to actively understanding and applying responsible AI principles in practice. Want to get started exploring this topic furthere? Bookmark and revisit these three core resources:\\n\\n| Resource | Description |\\n|:---|:---|\\n| 1\ufe0f\u20e3 [Blog Post](https://azure.microsoft.com/en-us/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/) | Official Announcement of New Tools |\\n| 2\ufe0f\u20e3 [Collection](https://aka.ms/rai-hub/collection)| My Responible AI For Developers Collection |\\n| 3\ufe0f\u20e3 [Collection](https://aka.ms/ai-studio/collection)| My Azure AI For Developers Collection|\\n\\n---"},{"id":"cookbook-kickoff","metadata":{"permalink":"/responsible-ai/cookbook-kickoff","source":"@site/blog/2024-03-31/index.md","title":"My Responsible AI Cookbook","description":"Responsible AI principles and practices are even more critical in the current fast-paced Generative AI ecosystem. This is the start of my learning journey to build a Responsible AI cookbook with useful recipes for generative AI developers.","date":"2024-03-31T00:00:00.000Z","formattedDate":"March 31, 2024","tags":[{"label":"responsibleai","permalink":"/responsible-ai/tags/responsibleai"},{"label":"principles","permalink":"/responsible-ai/tags/principles"},{"label":"tools","permalink":"/responsible-ai/tags/tools"},{"label":"practices","permalink":"/responsible-ai/tags/practices"},{"label":"recipes","permalink":"/responsible-ai/tags/recipes"}],"readingTime":5.045,"hasTruncateMarker":false,"authors":[{"name":"Nitya Narasimhan","title":"PhD & Polyglot, AI @Microsoft","url":"https://github.com/nitya","imageURL":"https://github.com/nitya.png","key":"nitya"}],"frontMatter":{"slug":"cookbook-kickoff","title":"My Responsible AI Cookbook","description":"Responsible AI principles and practices are even more critical in the current fast-paced Generative AI ecosystem. This is the start of my learning journey to build a Responsible AI cookbook with useful recipes for generative AI developers.","authors":["nitya"],"keywords":["azure","ai-studio","responsible-ai","generative-ai","rai-dashboard","rai-evaluation","rai-content-safety","rai-cookbook"],"tags":["responsibleai","principles","tools","practices","recipes"]},"unlisted":false,"prevItem":{"title":"Responsible AI On Azure","permalink":"/responsible-ai/twain-apr01-2024"}},"content":"Welcome to the kickoff of my latest project - a Responsible AI Cookbook. This is a personal learning project to explore the increasingly critical area of responsible AI - from core concepts and principles, to developer tools and workflows - with specific focus on the **generative AI applications** ecosystem. In this first post, I\'ll set the stage by defining a roadmap for what I want to learn, and explaining why I want to use the #30Days model for building a cookbook for developers.\\n\\n\\n\\n## What This Post Covers:\\n * What is Responsible AI and why learn about it?\\n * What is my #30DaysOf Learning Roadmap?\\n * What does the Cookbook format provide?\\n * How does this relate to Generative AI?\\n * Resource Collection: [**Responsible AI For Developers**](https://aka.ms/rai-hub/collection)\\n\\n![Deploy with Responsible AI](./../../static/img/banners/000-cookbook.png)\\n\\n---\\n\\n## 1. Background\\n\\nAs an AI Advocate at Microsoft, I\'ve spent the last six months exploring different aspects of the Generative AI landscape - from [Prompt Engineering](https://dev.to/azure/prompt-engineering-fundamentals-generative-ai-for-beginners-v1-1kii) and [Fine Tuning](https://dev.to/azure/fine-tuning-fundamentals-generative-ai-for-beginners-v2-3lf9) fundamentals to [Code-first Development](https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/a-code-first-experience-for-building-a-copilot-with-azure-ai/ba-p/4058659) using Azure AI Studio. But it wasn\'t till I started working on a [Responsible AI Hub For Developers](https://dev.to/azure/responsible-ai-for-developers-resources-for-self-guided-learning-3lf9) that I realized how _responsible AI_ became critical to every step of the end-to-end development workflow (LLM Ops) for generative AI solutions. \\n\\nAnd while there was a lot of information about responsible AI, it was scattered across different locations - from open-source toolboxes for data scientists, to cloud-based integrations for operationalization, and community-based forums for discussing real-world case studies and best practices. I needed a way to explore this in a _structured_ manner, and build myself a \\"cookbook\\" of useful recipes that I could revisit to find the right tool or process for a given task.\\n\\n\\n## 2. What is Responsible AI?\\n\\nResponsible AI is a [fundamental approach](https://responsibleaitoolbox.ai/) to designing, developing, assessing, and deploying, AI solutions in a **safe, trustworthy and ethical manner**. Microsoft has an organization-wide [Responsible AI](https://www.microsoft.com/ai/responsible-ai) standard and playbook anchored by six principles:\\n1. **Fairness** - AI systems should treat all people fairly.\\n1. **Reliability and Safety** - AI systems should perform reliably and safely (even under unplanned conditions).\\n1. **Privacy and Security** - AI systems should be secure and respect privacy (think system security and user data privacy)\\n1. **Inclusiveness** - AI systems should empower everyone and engage people (think about accessible experiences)\\n1. **Transparency** - AI systems should be understandable. (think about interpretability and explainability for decisions)\\n1. Accountability - People should be accountable for AI systems (think about oversight and guardrails for ensuring compliance)\\n\\n![Principles of Responsible AI](./../../static/img/posts/000-responsibleai-principles.png)\\n\\nMy perspective here is that we need to _shift left_ in the application lifecycle when it comes to detecting and mitigating or eliminating potential harms that can adversely impact responsible AI usage. And that means raising **developer awareness** of these issues and documenting **actionable recipes** that can be integrated into their design thinking and developer workflows.\\n\\n\\n## 3. My #30Days Roadmap\\n\\nMy #30DaysOf platform and process started off as a way to create _themed month_ content to give learners a deep-dive into a given topic in the span of four weeks. However, I want to repurpose the format for _self-paced learning_ by keeping the same structure (roadmap) but without the time pressure for completion (30 days). The _Cookbook_ format seemed to work best:\\n\\n1. The #30Days site becomes a developer cookbook for a given topic.\\n1. The Roadmap is the table-of-contents, organized as top-level collections.\\n1. The Collection is a set of recipes focused on a particular sub-topic.\\n1. Each Recipe has an explainer (what is it) and an exercise (how do I use it).\\n\\nMy immediate goal is to build this cookbook **for my own use** - all mistakes and misconceptions are mine and the content is intended for learning purposes only and not for production use. With that in mind, my first steps are to find all relevant learning resources and organize them into a hierarchical roadmap (tree of nodes) with the following top-level nodes:\\n - Core Concepts - understand key terminology and building blocks\\n - Developer Tools - explore key tools and workflows for developers\\n - Generative AI - understand Responsible AI usage for LLM Ops\\n - Advanced Topics - covers research, best practices & new projects.\\n\\n\\n## 4. Generative AI and LLM Ops\\n\\nTraditional AI applications focused on training models with domain-specific datasets, and deploying _predictive AI_ endpoints that applications could use for decision-making. Modern _generative AI_ applications focus on pre-trained \\"large language models\\" trained on massive internet-scale data, with endpoints that applications can use for generating original content (text, code, images, audio, video etc.). \\n\\nThis unlocks a whole new range of user experiences driven by _natural language_ interactions (chat), driving a paradigm shift from MLOps to LLMOps. In my day job as an AI Advocate at Microsoft, I focus on how we can use the [Azure AI platform](https://ai.azure.com) to streamline the end-to-end development of Generative AI applications - and use this figure to explain what the LLM Ops (or end-to-end application lifecycle) would involve.\\n\\n![LLM Ops Workflow](https://techcommunity.microsoft.com/t5/image/serverpage/image-id/552316i571E7F652BBB7F1C/image-size/large?v=v2&px=999)\\n\\nWhen we think about Responsible AI in the context of this workflow, we have to [plan a responsible generative AI solution](https://learn.microsoft.com/training/modules/responsible-generative-ai/2-plan-responsible-ai) with 4 steps:\\n - _Identify_ potential harms relevant to your solution\\n - _Evaluate_ presence of harms in generated outputs\\n - _Mitigate_ harms at multiple levels to minimize impact\\n - _Operate_ solution responsibly with an ops readiness plan.\\n\\n With the Azure AI platform, this is captured in the [approach to generative AI evaluations](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-approach-gen-ai), the [responsible AI guidelines for Azure AI services](https://learn.microsoft.com/azure/ai-services/responsible-use-of-ai-overview) and the [content filtering system](https://learn.microsoft.com/azure/ai-studio/concepts/content-filtering)for deployments. \\n\\n![Evaluation](https://learn.microsoft.com/en-us/azure/ai-studio/media/evaluations/evaluation-monitor-flow.png)\\n\\nThe figure above gives us a high-level sense for where these mechanisms fit against the three stages in the LLM Ops diagram above. And just last week, the team announced [new tools for building secure and trustworthy generative AI applications](https://azure.microsoft.com/blog/announcing-new-tools-in-azure-ai-to-help-you-build-more-secure-and-trustworthy-generative-ai-applications/) that enhance the workflow even more to tackle challenges like prompt injection attacks, vulnerability to jailbreaking, detection of \\"hallucinations\\" and more.\\n\\n\\n\\n## 5. Next Steps\\n\\nMy next step is to build a roadmap (table of contents with links to relevant resources) to help me plan out my learning journey. Stay tuned!"}]}')}}]);